#!/usr/bin/env python3
"""
Test de Centralisation Configuration Compl√®te

V√©rifie que toute la configuration est centralis√©e dans config.json
sans redondance entre les fichiers.
"""

import json
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.prompt_manager import PromptManager


def test_configuration_centralization():
    """Test principal de centralisation"""
    print("\nüéØ TEST CENTRALISATION CONFIGURATION")
    print("=" * 50)
    
    success = True
    
    # 1. V√©rifier que config.json contient TOUS les settings LLM
    print("\n1Ô∏è‚É£ Configuration centralis√©e dans config.json")
    try:
        with open("config/config.json", 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        content_gen = config_data.get("content_generation", {})
        
        # V√©rifier auto_reply settings
        auto_reply = content_gen.get("auto_reply", {})
        required_auto_reply = ["provider", "model", "max_tokens", "temperature", "force_llm"]
        
        print("   üìã Auto-reply settings:")
        for key in required_auto_reply:
            if key in auto_reply:
                print(f"      ‚úÖ {key}: {auto_reply[key]}")
            else:
                print(f"      ‚ùå {key}: MANQUANT")
                success = False
        
        # V√©rifier tweet_generation settings  
        tweet_gen = content_gen.get("tweet_generation", {})
        required_tweet = ["provider", "model", "max_tokens", "temperature"]
        
        print("   üê¶ Tweet generation settings:")
        for key in required_tweet:
            if key in tweet_gen:
                print(f"      ‚úÖ {key}: {tweet_gen[key]}")
            else:
                print(f"      ‚ùå {key}: MANQUANT")
                success = False
                
    except Exception as e:
        print(f"   ‚ùå Erreur lecture config.json: {e}")
        success = False
    
    # 2. V√©rifier que prompts.json ne contient PLUS de settings
    print("\n2Ô∏è‚É£ √âlimination redondance dans prompts.json")
    try:
        with open("config/prompts.json", 'r', encoding='utf-8') as f:
            prompts_data = json.load(f)
        
        if "settings" in prompts_data:
            print("   ‚ùå ERREUR: Section 'settings' encore pr√©sente dans prompts.json")
            print(f"      ‚Üí Settings trouv√©s: {list(prompts_data['settings'].keys())}")
            success = False
        else:
            print("   ‚úÖ Section 'settings' supprim√©e de prompts.json")
        
        # V√©rifier que les prompts sont toujours l√†
        required_sections = ["system_prompts", "user_prompts", "templates"]
        for section in required_sections:
            if section in prompts_data:
                print(f"   ‚úÖ {section}: {len(prompts_data[section])} √©l√©ments")
            else:
                print(f"   ‚ùå {section}: MANQUANT")
                success = False
                
    except Exception as e:
        print(f"   ‚ùå Erreur lecture prompts.json: {e}")
        success = False
    
    # 3. Tester PromptManager avec la nouvelle architecture
    print("\n3Ô∏è‚É£ Test PromptManager centralis√©")
    try:
        pm = PromptManager()
        
        # Test lecture settings depuis config.json
        auto_reply_settings = pm.get_setting("auto_reply")
        if auto_reply_settings and "force_llm" in auto_reply_settings:
            print(f"   ‚úÖ Auto-reply settings: {auto_reply_settings}")
        else:
            print(f"   ‚ùå √âchec lecture auto_reply settings")
            success = False
        
        # Test lecture prompts depuis prompts.json
        system_prompt = pm.get_system_prompt("auto_reply")
        if system_prompt and "content" in system_prompt:
            print(f"   ‚úÖ System prompts: OK")
        else:
            print(f"   ‚ùå √âchec lecture system prompts")
            success = False
        
        # Test templates
        templates = pm.get_template("crypto_topics")
        if templates and len(templates) > 0:
            print(f"   ‚úÖ Templates: {len(templates)} crypto topics")
        else:
            print(f"   ‚ùå √âchec lecture templates")
            success = False
            
    except Exception as e:
        print(f"   ‚ùå Erreur PromptManager: {e}")
        success = False
    
    # 4. V√©rifier coh√©rence des param√®tres
    print("\n4Ô∏è‚É£ Coh√©rence des param√®tres")
    try:
        # V√©rifier que les mod√®les sont coh√©rents
        auto_reply_model = pm.get_setting("auto_reply", "model")
        tweet_model = pm.get_setting("tweet_generation", "model")
        
        if auto_reply_model == tweet_model:
            print(f"   ‚úÖ Mod√®les coh√©rents: {auto_reply_model}")
        else:
            print(f"   ‚ö†Ô∏è Mod√®les diff√©rents: auto_reply={auto_reply_model}, tweet={tweet_model}")
        
        # V√©rifier provider
        auto_reply_provider = pm.get_setting("auto_reply", "provider")
        tweet_provider = pm.get_setting("tweet_generation", "provider")
        
        if auto_reply_provider == tweet_provider:
            print(f"   ‚úÖ Providers coh√©rents: {auto_reply_provider}")
        else:
            print(f"   ‚ö†Ô∏è Providers diff√©rents: auto_reply={auto_reply_provider}, tweet={tweet_provider}")
            
    except Exception as e:
        print(f"   ‚ùå Erreur v√©rification coh√©rence: {e}")
        success = False
    
    # 5. Test int√©gration compl√®te
    print("\n5Ô∏è‚É£ Test int√©gration compl√®te")
    try:
        # Simuler utilisation r√©elle
        system_prompt = pm.get_system_prompt("auto_reply")
        user_prompt = pm.get_user_prompt("auto_reply", username="TestUser", reply_content="Great tweet!")
        settings = pm.get_setting("auto_reply")
        
        print(f"   ‚úÖ System prompt: {len(system_prompt['content'])} caract√®res")
        print(f"   ‚úÖ User prompt: {len(user_prompt)} caract√®res")
        print(f"   ‚úÖ Settings: {len(settings)} param√®tres")
        
        # V√©rifier force_llm
        if settings.get("force_llm") is True:
            print(f"   ‚úÖ force_llm activ√© pour GPT/LLM")
        else:
            print(f"   ‚ùå force_llm non configur√©")
            success = False
            
    except Exception as e:
        print(f"   ‚ùå Erreur test int√©gration: {e}")
        success = False
    
    # R√©sultat final
    print(f"\n{'='*50}")
    if success:
        print("üéâ CENTRALISATION R√âUSSIE!")
        print("‚úÖ Configuration unifi√©e dans config.json")
        print("‚úÖ Prompts isol√©s dans prompts.json")
        print("‚úÖ Aucune redondance d√©tect√©e")
        print("‚úÖ PromptManager fonctionnel")
        return True
    else:
        print("‚ùå CENTRALISATION √âCHOU√âE!")
        print("‚ö†Ô∏è V√©rifier les erreurs ci-dessus")
        return False


def test_performance():
    """Test performance apr√®s centralisation"""
    print("\n‚ö° TEST PERFORMANCE")
    print("=" * 30)
    
    import time
    
    start_time = time.time()
    pm = PromptManager()
    init_time = time.time() - start_time
    
    start_time = time.time()
    for i in range(100):
        settings = pm.get_setting("auto_reply")
        prompt = pm.get_system_prompt("auto_reply")
    access_time = time.time() - start_time
    
    print(f"‚ö° Init time: {init_time:.3f}s")
    print(f"‚ö° 100 acc√®s: {access_time:.3f}s")
    print(f"‚ö° Moyenne: {access_time/100*1000:.2f}ms par acc√®s")
    
    if init_time < 0.1 and access_time < 0.1:
        print("‚úÖ Performance excellente")
        return True
    else:
        print("‚ö†Ô∏è Performance d√©grad√©e")
        return False


def show_final_structure():
    """Affiche la structure finale centralis√©e"""
    print("\nüìÅ STRUCTURE FINALE CENTRALIS√âE")
    print("=" * 40)
    
    print("\nüìÑ config/config.json (CONFIGURATION):")
    print("   ‚îú‚îÄ‚îÄ content_generation/")
    print("   ‚îÇ   ‚îú‚îÄ‚îÄ provider: auto")
    print("   ‚îÇ   ‚îú‚îÄ‚îÄ model: gpt-4o-mini")
    print("   ‚îÇ   ‚îú‚îÄ‚îÄ auto_reply/")
    print("   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ force_llm: true")
    print("   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temperature: 0.9")
    print("   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ max_tokens: 60")
    print("   ‚îÇ   ‚îî‚îÄ‚îÄ tweet_generation/")
    print("   ‚îÇ       ‚îú‚îÄ‚îÄ temperature: 0.7")
    print("   ‚îÇ       ‚îî‚îÄ‚îÄ max_tokens: 150")
    print("   ‚îú‚îÄ‚îÄ engagement/")
    print("   ‚îú‚îÄ‚îÄ monitoring/")
    print("   ‚îî‚îÄ‚îÄ storage/")
    
    print("\nüìÑ config/prompts.json (PROMPTS SEULEMENT):")
    print("   ‚îú‚îÄ‚îÄ system_prompts/")
    print("   ‚îÇ   ‚îú‚îÄ‚îÄ tweet_generation")
    print("   ‚îÇ   ‚îî‚îÄ‚îÄ auto_reply")
    print("   ‚îú‚îÄ‚îÄ user_prompts/")
    print("   ‚îÇ   ‚îú‚îÄ‚îÄ tweet_generation")
    print("   ‚îÇ   ‚îî‚îÄ‚îÄ auto_reply")
    print("   ‚îî‚îÄ‚îÄ templates/")
    print("       ‚îú‚îÄ‚îÄ crypto_topics")
    print("       ‚îú‚îÄ‚îÄ simple_replies")
    print("       ‚îî‚îÄ‚îÄ image_themes")
    
    print("\nüéØ AVANTAGES:")
    print("   ‚úÖ Configuration centralis√©e")
    print("   ‚úÖ Aucune duplication")
    print("   ‚úÖ Maintenance simplifi√©e")
    print("   ‚úÖ Source unique de v√©rit√©")


if __name__ == "__main__":
    print("üîß TEST CENTRALISATION CONFIGURATION COMPL√àTE")
    print("="*60)
    
    # Tests principaux
    config_ok = test_configuration_centralization()
    perf_ok = test_performance()
    
    # Affichage structure
    show_final_structure()
    
    # R√©sultat global
    print(f"\n{'='*60}")
    if config_ok and perf_ok:
        print("üéâ CENTRALISATION COMPL√àTE R√âUSSIE!")
        print("üöÄ Le syst√®me est pr√™t avec configuration unifi√©e!")
    else:
        print("‚ùå PROBL√àMES D√âTECT√âS")
        print("‚ö†Ô∏è V√©rifier les erreurs ci-dessus") 