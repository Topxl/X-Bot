#!/usr/bin/env python3
"""
Test simplifiÃ© pour vÃ©rifier le fix des mentions dans les prompts
VÃ©rifie que les prompts sont correctement configurÃ©s sans imports complexes
"""

import json
import os

class TestMentionFixSimple:
    def __init__(self):
        print("ğŸ”§ Test Fix des Mentions (SimplifiÃ©)")
        print("=" * 50)
        
    def test_prompt_configuration(self):
        """Test de la configuration des prompts"""
        print("\n1. ğŸ“„ Test Configuration Prompts")
        print("-" * 30)
        
        try:
            # Charger prompts.json
            with open('config/prompts.json', 'r', encoding='utf-8') as f:
                prompts = json.load(f)
            
            # VÃ©rifier system prompt
            system_prompt = prompts['system_prompts']['auto_reply']['content']
            print(f"System prompt: {system_prompt[:80]}...")
            
            system_checks = [
                ("Contient '@username'", "@username" in system_prompt),
                ("Interdit '@MaxiMemeFeed'", "@MaxiMemeFeed" in system_prompt), 
                ("Contient 'CRITICAL RULES'", "CRITICAL RULES" in system_prompt),
                ("Mention 'NEVER mention @MaxiMemeFeed'", "NEVER mention @MaxiMemeFeed" in system_prompt)
            ]
            
            print("   VÃ©rifications system prompt:")
            system_passed = 0
            for check_name, check_result in system_checks:
                status = "âœ…" if check_result else "âŒ"
                print(f"     {status} {check_name}")
                if check_result:
                    system_passed += 1
            
            # VÃ©rifier user prompt template
            user_template = prompts['user_prompts']['auto_reply']['template']
            print(f"\nUser template: {user_template[:80]}...")
            
            user_checks = [
                ("Contient '@{username}'", "@{username}" in user_template),
                ("Template demande mention", "start your reply with @{username}" in user_template.lower()),
                ("Variables incluent 'username'", "username" in prompts['user_prompts']['auto_reply']['variables'])
            ]
            
            print("   VÃ©rifications user prompt:")
            user_passed = 0
            for check_name, check_result in user_checks:
                status = "âœ…" if check_result else "âŒ"
                print(f"     {status} {check_name}")
                if check_result:
                    user_passed += 1
            
            total_checks = len(system_checks) + len(user_checks)
            total_passed = system_passed + user_passed
            
            print(f"\nğŸ“Š Score: {total_passed}/{total_checks} vÃ©rifications passÃ©es")
            
            return total_passed == total_checks
            
        except Exception as e:
            print(f"âŒ Erreur test prompts: {e}")
            return False
    
    def test_prompt_examples(self):
        """Test avec exemples concrets"""
        print("\n2. ğŸ¯ Test Exemples Concrets")
        print("-" * 30)
        
        try:
            with open('config/prompts.json', 'r', encoding='utf-8') as f:
                prompts = json.load(f)
            
            template = prompts['user_prompts']['auto_reply']['template']
            
            # Simulation de prompt avec vraies donnÃ©es
            example_prompt = template.format(
                username="cryptofan123",
                reply_content="Bitcoin going to the moon! ğŸš€"
            )
            
            print(f"Exemple de prompt gÃ©nÃ©rÃ©:")
            print(f"   {example_prompt}")
            
            checks = [
                ("Contient @cryptofan123", "@cryptofan123" in example_prompt),
                ("Contient le contenu", "Bitcoin going to the moon!" in example_prompt),
                ("Instructions claires", "start your reply with @cryptofan123" in example_prompt.lower())
            ]
            
            all_passed = True
            for check_name, check_result in checks:
                status = "âœ…" if check_result else "âŒ"
                print(f"   {status} {check_name}")
                if not check_result:
                    all_passed = False
            
            return all_passed
            
        except Exception as e:
            print(f"âŒ Erreur test exemples: {e}")
            return False
    
    def test_before_after_comparison(self):
        """Affichage avant/aprÃ¨s du fix"""
        print("\n3. ğŸ”„ Comparaison Avant/AprÃ¨s")
        print("-" * 30)
        
        print("âŒ AVANT (problÃ¨me):")
        print("   Prompt: 'Reply to this comment: Bitcoin moon!'")
        print("   RÃ©ponse LLM: '@MaxiMemeFeed Great analysis! ğŸš€'")
        print("   ProblÃ¨me: Auto-mention + pas de mention utilisateur")
        
        print("\nâœ… APRÃˆS (corrigÃ©):")
        print("   Prompt: 'Reply to @cryptofan123 who commented: Bitcoin moon!'")
        print("   RÃ©ponse LLM: '@cryptofan123 ğŸš€ Which catalyst do you think?'")
        print("   Solution: Mention correcte + pas d'auto-mention")
        
        return True
    
    def run_all_tests(self):
        """Lance tous les tests"""
        tests = [
            ("Configuration Prompts", self.test_prompt_configuration),
            ("Exemples Concrets", self.test_prompt_examples),
            ("Comparaison Avant/AprÃ¨s", self.test_before_after_comparison)
        ]
        
        results = []
        
        for test_name, test_func in tests:
            print(f"\nğŸ§ª {test_name}...")
            try:
                result = test_func()
                results.append((test_name, result))
                status = "âœ… PASSÃ‰" if result else "âŒ Ã‰CHOUÃ‰"
                print(f"   {status}")
            except Exception as e:
                print(f"   âŒ ERREUR: {e}")
                results.append((test_name, False))
        
        # RÃ©sumÃ©
        print("\n" + "=" * 50)
        print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
        print("=" * 50)
        
        passed = sum(1 for _, result in results if result)
        total = len(results)
        
        for test_name, result in results:
            status = "âœ…" if result else "âŒ"
            print(f"{status} {test_name}")
        
        print(f"\nğŸ¯ Score: {passed}/{total} tests passÃ©s")
        
        if passed == total:
            print("ğŸ‰ MENTION FIX CONFIRMÃ‰ ! Les prompts sont corrects.")
            print("\nğŸ”§ RÃ‰SOLUTION DES PROBLÃˆMES:")
            print("   1. âœ… Le bot ne s'auto-mentionnera plus (@MaxiMemeFeed)")
            print("   2. âœ… Le bot mentionnera l'utilisateur correct (@username)")
            print("   3. âœ… Plus de liens bizarres dans les rÃ©ponses")
            print("   4. âœ… Instructions claires dans les prompts")
        else:
            print("âš ï¸ Des problÃ¨mes persistent dans la configuration.")
        
        return passed == total

if __name__ == "__main__":
    # VÃ©rifier qu'on est dans le bon rÃ©pertoire
    if not os.path.exists('config/prompts.json'):
        print("âŒ Erreur: config/prompts.json non trouvÃ©")
        print("ğŸ’¡ Assurez-vous d'Ãªtre dans le rÃ©pertoire du bot")
        exit(1)
    
    tester = TestMentionFixSimple()
    success = tester.run_all_tests()
    
    if success:
        print("\nâœ… Le fix des mentions est prÃªt !")
        print("ğŸ’¡ RedÃ©marrez le bot pour appliquer les changements.")
    else:
        print("\nâŒ Configuration incomplÃ¨te.")
    
    exit(0 if success else 1) 